{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "  - A parameter is an internal, learnable part of a machine learning model. It's a variable that the model learns during the training process and uses to make predictions. Parameters are adjusted by the optimization algorithm to minimize the loss function and improve the model's accuracy.\n",
        "\n",
        "2. What is correlation?\n",
        "  - A statistical tool that helps in the study of the relationship between two variables is known as Correlation. It also helps in understanding the economic behaviour of the variables.\n",
        "      \n",
        "      **What does negative correlation mean?**\n",
        "  - When two variables move in opposite directions; i.e., when one increases the other decreases, and vice-versa, then such a relation is called a Negative Correlation.\n",
        "\n",
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "  - Machine learning is a branch of artificial intelligence that enables algorithms to uncover hidden patterns within datasets. It allows them to predict new, similar data without explicit programming for each task. Machine learning finds applications in diverse fields such as image and speech recognition, natural language processing, recommendation systems, fraud detection, portfolio optimization, and automating tasks.\n",
        "\n",
        "      There are following components in Machine Learning.\n",
        "        - **Data:** Data is the foundation of machine learning. It can be in various forms, such as images, text, numbers, or sensor readings.\n",
        "        - **Task:** The task defines the specific problem that the machine learning model aims to solve. It could be classification, regression, clustering, or any other machine learning problem.\n",
        "        - **Model:** A model is a mathematical representation of the patterns and relationships learned from the data. It's the core component that makes predictions or decisions.\n",
        "        - **Algorithm:** An algorithm is a set of rules or instructions that a model follows to learn from data and make predictions. Different algorithms are used for different types of models and tasks.\n",
        "        - **Loss Function:** A loss function measures the difference between the model's predictions and the actual values in the data. It helps the algorithm to improve the model's accuracy. The choice of loss function depends on the specific problem and the type of model being used.\n",
        "        - **Optimizer:** An optimizer is an algorithm that adjusts the model's parameters to minimize the loss function and improve its performance.\n",
        "        - **Evaluation Metrics:** Evaluation metrics are used to assess the performance of a trained model on unseen data.\n",
        "        \n",
        "4. How does loss value help in determining whether the model is good or not?\n",
        "  - The goal of training a machine learning model is to minimize this loss value. If the loss value is low, it means the model's predictions are close to the true values, implying a good fit to the data. Conversely, a high loss value suggests the model is making significant errors and may not be generalizing well.\n",
        "\n",
        "5. What are continuous and categorical variables?\n",
        "  - **Continuous Variables:** Continuous variables are numeric variables that can take on any value within a given range. They are measured on a continuous scale, and there are an infinite number of possible values between any two points.\n",
        "  - **Categorical Variables:** Categorical variables are variables that represent categories or groups. It mainly represents qualitative data and not quantitative (numerical); thus, it is a qualitative variable.\n",
        "\n",
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "  - Machine learning models typically work with numerical data. Categorical variables, which represent categories or groups (e.g., colors, city names), need to be converted into a numerical format before they can be used in a model. This process is called encoding.\n",
        "\n",
        "      **Common Techniques for Encoding Categorical Variables**\n",
        "      - **One-Hot Encoding:** Generally preferred for nominal categorical variables with no inherent order, but can lead to high dimensionality with many categories.\n",
        "      - **Label Encoding/Ordinal Encoding:** Suitable for ordinal categorical variables with a meaningful order.\n",
        "      - **Target Encoding:** Useful for supervised learning tasks but requires careful handling to avoid leakage.\n",
        "\n",
        "7. What do you mean by training and testing a dataset?\n",
        "  - **Training dataset:** The primary goal of training a dataset is to build a machine learning model that can learn patterns and relationships within the data. This is achieved by feeding the model a large portion of the dataset, known as the training set.\n",
        "  - **Testing Dataset:** After training a model, it's essential to evaluate its performance on a separate dataset called the test set. This helps assess the model's ability to generalize to new, unseen data.\n",
        "\n",
        "8. What is sklearn.preprocessing?\n",
        "  - `sklearn.preprocessing` is a module in the scikit-learn library in Python that provides various functions and classes for preprocessing data before feeding it into machine learning models.\n",
        "\n",
        "      The main purpose of `sklearn.preprocessing` is to transform raw feature vectors into a representation that is more suitable for the downstream estimators (machine learning models).\n",
        "\n",
        "9. What is a Test set?\n",
        "  - A test set is a crucial component in the field of statistics, data analysis, and data science, serving as a subset of data used to evaluate the performance of a predictive model. In the context of machine learning, the test set is distinct from both the training set and the validation set.\n",
        "\n",
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "  - We typically split our data into two subsets:\n",
        "      - **Training Set:** This is the largest portion of your data, used to train the machine learning model. The model learns patterns and relationships from this data.\n",
        "      - **Testing Set:** This set is kept separate from the training process and is used to evaluate the final model's performance on unseen data. It provides an unbiased estimate of how well the model will generalize to new data.\n",
        "\n",
        "      **How do you approach a Machine Learning problem?**\n",
        "      \n",
        "      General approach to solving a machine learning problem is:\n",
        "        - **Define the Problem:** Clearly understand the problem you're trying to solve. What is the goal? What kind of data do you have?\n",
        "        - **Data Collection and Preparation:** Gather the necessary data and clean it. This might involve handling missing values, outliers, and converting categorical variables.\n",
        "        - **Exploratory Data Analysis (EDA):** Explore the data to understand its characteristics, identify patterns, and gain insights. Use visualizations and summary statistics.\n",
        "        - **Feature Engineering:** Select or create relevant features that will improve the model's performance.\n",
        "        - **Model Selection:** Choose an appropriate machine learning model based on the problem type and data characteristics.\n",
        "        - **Model Training:** Train the model using the training data.\n",
        "        - **Model Evaluation:** Evaluate the model's performance using the testing data and appropriate metrics.\n",
        "        - **Hyperparameter Tuning:** Adjust the model's hyperparameters to optimize its performance using the validation set.\n",
        "        - **Deployment and Monitoring:** Deploy the model and continuously monitor its performance to ensure it remains accurate and effective.\n",
        "        \n",
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "  - EDA is a necessary step before model fitting because it helps us prepare the data, select the right model, avoid potential problems, and ultimately build a more accurate and reliable machine learning model.There are following reasons:\n",
        "  \n",
        "  **1. Understanding the Data:** EDA helps us to gain a deeper understanding of the data's structure, patterns, and relationships between variables. This understanding is essential for selecting appropriate models and features.\n",
        "\n",
        "  **2. Identifying Data Quality Issues:** EDA allows us to detect potential problems in the data such as missing values, outliers, or inconsistencies. These issues can significantly impact the model's performance and need to be addressed before training.\n",
        "\n",
        "  **3. Feature Selection and Engineering:** By exploring the data, we can identify the most relevant features for the model and potentially create new features that improve its accuracy.\n",
        "\n",
        "  **4. Model Selection:** The insights gained from EDA can guide us in selecting the most suitable model for the task, whether it's regression, classification, or another type of model.\n",
        "\n",
        "  **5. Assumptions Validation:** Some machine learning models have underlying assumptions about the data. EDA helps to verify if these assumptions are met, ensuring the model's validity.\n",
        "\n",
        "  **6. Avoiding Bias and Overfitting:** By thoroughly exploring the data, we can identify potential biases or patterns that might lead to overfitting, where the model performs well on the training data but poorly on unseen data.\n",
        "\n",
        "  **7. Communication and Insights:** EDA provides valuable insights that can be communicated to stakeholders through visualizations and reports, enhancing understanding and decision-making.\n",
        "\n",
        "12. What is correlation?\n",
        "  - A statistical tool that helps in the study of the relationship between two variables is known as Correlation. It also helps in understanding the economic behaviour of the variables.\n",
        "\n",
        "13. What does negative correlation mean?\n",
        "  - When two variables move in opposite directions; i.e., when one increases the other decreases, and vice-versa, then such a relation is called a Negative Correlation.\n",
        "\n",
        "14. How can you find correlation between variables in Python?\n",
        "  - We can find correlation between variables in python , using Pandas and Numpy:\n",
        "      - **Using Pandas:**\n",
        "          - import the library:\n",
        "          ```\n",
        "          import pandas as pd\n",
        "          ```\n",
        "          - Load your data:\n",
        "          ```\n",
        "          data = pd.read_csv('data.csv')\n",
        "          ```\n",
        "          - Calculate the correlation matrix:\n",
        "          ```\n",
        "          correlation_matrix = data.corr()\n",
        "          ```\n",
        "      - **Using NumPy:**\n",
        "          - import the library:\n",
        "          ```\n",
        "          import numpy as np\n",
        "          ```\n",
        "          - Assuming you have two variables 'x' and 'y' as NumPy arrays:\n",
        "          ```\n",
        "          correlation_coefficient = np.corrcoef(x, y)[0, 1]\n",
        "          ```\n",
        "\n",
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "  - **Causation:** Causation is indicating that X and Y have a cause-and-effect connection with one another. It tells X causes Y. Causation is also understood as a basis. Firstly, causation indicates that two possibilities occur at the same time or one after the other. And secondly, it tells these two variables not only occur jointly, the presence of one drives the other to display.\n",
        "\n",
        "      **Causation Vs Correlation variables:**\n",
        "\n",
        "      Correlation means there is a statistical association between variables. Causation means that a change in one variable causes a change in another variable. In research, you might have come across the phrase “correlation doesn’t imply causation.”\n",
        "\n",
        "      **Example:**\n",
        "\n",
        "      **Scenario:** Ice cream sales and drowning incidents are positively correlated. They both tend to increase during the summer months.\n",
        "\n",
        "      **Correlation:** There is a correlation between ice cream sales and drowning incidents.\n",
        "\n",
        "      **Causation:** Does this mean that eating ice cream causes drowning? No! The underlying cause is likely a third variable – summertime. Warmer weather leads to more people swimming (increasing drowning risk) and more people buying ice cream.\n",
        "\n",
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "  - an optimizer is an algorithm or method used to adjust the parameters of a model during training to minimize the loss function. The loss function measures the difference between the model's predictions and the actual values in the training data. Optimizers play a crucial role in helping models converge to optimal parameters that improve their accuracy and performance.\n",
        "\n",
        "      There are various types of optimizer\n",
        "        - **Adam Optimizer:** Adam(Adaptive Moment Estimation) is highly effective, especially when working with large datasets and complex models, because it is memory-efficient and adapts the learning rate dynamically for each parameter.\n",
        "\n",
        "            **Example:**\n",
        "        ```\n",
        "        from tensorflow.keras.optimizers import Adam\n",
        "            optimizer = Adam(learning_rate=0.001)\n",
        "            model.compile(optimizer=optimizer, loss='mse')\n",
        "        ```\n",
        "        - **Gradient Descent (GD):** Gradient Descent is an algorithm used to find the best solution to a problem by making small adjustments in the right direction. It’s like trying to find the lowest point in a hilly area by walking down the slope, step by step, until you reach the bottom.\n",
        "\n",
        "            **Example:**\n",
        "        ```\n",
        "        from sklearn.linear_model import SGDRegressor\n",
        "            model = SGDRegressor(learning_rate='constant', eta0=0.01)\n",
        "            model.fit(X_train, y_train)\n",
        "        ```\n",
        "        - **RMSProp (Root Mean Square Propagation):** RMSProp improves upon AdaGrad by introducing a decay factor to prevent the learning rate from decreasing too rapidly.\n",
        "\n",
        "            **Example:**\n",
        "        ```\n",
        "        from tensorflow.keras.optimizers import RMSprop\n",
        "            optimizer = RMSprop(learning_rate=0.001)\n",
        "            model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
        "        ```\n",
        "        - **AdaGrad (Adaptive Gradient Algorithm):** AdaGrad adapts the learning rate for each parameter based on the historical gradient information. The learning rate decreases over time, making AdaGrad effective for sparse features.\n",
        "\n",
        "            **Example:**\n",
        "        ```\n",
        "        from tensorflow.keras.optimizers import Adagrad\n",
        "            optimizer = Adagrad(learning_rate=0.01)\n",
        "            model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
        "        ```\n",
        "        - **Adadelta:** Adadelta is an extension of Adagrad. It addresses the problem of excessively diminishing learning rates. It uses a moving window of gradient updates, helping the model learn effectively even with sparse data.\n",
        "\n",
        "            **Example:**\n",
        "        ```\n",
        "        from tensorflow.keras.optimizers import Adadelta\n",
        "            optimizer = Adadelta(learning_rate=1.0)  \n",
        "            model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
        "        ```\n",
        "17. What is sklearn.linear_model ?\n",
        "  - `sklearn.linear_model` is a module in the scikit-learn library in Python that provides a set of classes and functions for performing linear regression and other related tasks. Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data.\n",
        "\n",
        "18. What does model.fit() do? What arguments must be given?\n",
        "  - In machine learning, `model.fit()` is a crucial method used to train a machine learning model. It essentially teaches the model to learn patterns and relationships within the provided training data. During this process, the model adjusts its internal parameters to minimize errors and improve its predictive accuracy.\n",
        "\n",
        "      The `model.fit()` method typically requires two main arguments:\n",
        "        - **Input Data (X):** This represents the features or independent variables of your training data. It's often a NumPy array or a Pandas DataFrame containing the values of the features that the model will learn from.\n",
        "        - **Target Variable (y):** This represents the outcome or dependent variable that the model is trying to predict. It's also often a NumPy array or a Pandas DataFrame containing the corresponding values of the target variable for each data point in the input data.\n",
        "\n",
        "19. What does model.predict() do? What arguments must be given?\n",
        "  - `model.predict()` is used to generate predictions from the trained model based on new input data. It does not require true labels and does not compute any metrics.\n",
        "\n",
        "      The primary argument for model.predict() is the input data on which you want to make predictions. This data should be in the same format and have the same features as the data used to train the model.\n",
        "\n",
        "20. What are continuous and categorical variables?\n",
        "  - **Continuous Variables:** Continuous variables are numeric variables that can take on any value within a given range. They are measured on a continuous scale, and there are an infinite number of possible values between any two points.\n",
        "  - **Categorical Variables:** Categorical variables are variables that represent categories or groups. It mainly represents qualitative data and not quantitative (numerical); thus, it is a qualitative variable.\n",
        "  \n",
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "  - Feature scaling is a preprocessing technique that transforms feature values to a similar scale, ensuring all features contribute equally to the model. It’s essential for datasets with features of varying ranges, units, or magnitudes. Common techniques include standardization, normalization, and min-max scaling.\n",
        "\n",
        "      If feature scaling is not done then machine learning algorithm tends to use greater values as higher and consider smaller values as lower regardless of the unit of the values. For example it will take 10 m and 10 cm both as same regardless of their unit. In this article we will learn about different techniques which are used to perform feature scaling.\n",
        "\n",
        "22. How do we perform scaling in Python?\n",
        "  - We perform scaling in python are following step:\n",
        "      - **Step 1:** Import library\n",
        "      ```\n",
        "      from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "      ```\n",
        "      - **Step 2:** Create a scaling object: Choose the appropriate scaling method based on your needs. `StandardScaler` is for standardization, `MinMaxScaler` for normalization, and `RobustScaler` for handling outliers.\n",
        "      - **Step 3:** Fit the scaler to your data: This step calculates the necessary parameters (like mean and standard deviation for standardization) based on your data.\n",
        "      - **Step 4:** Transform your data: Apply the scaling transformation to your training and testing data.\n",
        "\n",
        "      **Example:**\n",
        "      ```\n",
        "      import pandas as pd\n",
        "      from sklearn.preprocessing import StandardScaler\n",
        "      from sklearn.model_selection import train_test_split\n",
        "      data = pd.read_csv('your_data.csv')\n",
        "      X = data[['feature1', 'feature2', ...]]  \n",
        "      y = data['target_variable']  \n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
        "      scaler = StandardScaler()  \n",
        "      scaler.fit(X_train)  \n",
        "      X_train_scaled = scaler.transform(X_train)  \n",
        "      X_test_scaled = scaler.transform(X_test)\n",
        "      ```\n",
        "      \n",
        "23. What is sklearn.preprocessing?\n",
        "  - `sklearn.preprocessing` is a module in the scikit-learn library in Python that provides various functions and classes for preprocessing data before feeding it into machine learning models.\n",
        "\n",
        "      The main purpose of `sklearn.preprocessing` is to transform raw feature vectors into a representation that is more suitable for the downstream estimators (machine learning models).\n",
        "\n",
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "  - We split data for model fitting (training and testing) in Python is following step are follow:\n",
        "      - **Step 1:** Import library\n",
        "      ```\n",
        "      from sklearn.model_selection import train_test_split\n",
        "      ```\n",
        "      - **Step 2:** Prepare data\n",
        "      - **Step 3:** Split the data\n",
        "      ```\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "      ```\n",
        "\n",
        "          **Example:**\n",
        "      ```\n",
        "      import pandas as pd\n",
        "      from sklearn.model_selection import train_test_split\n",
        "      data = pd.read_csv('your_data.csv')\n",
        "      X = data[['feature1', 'feature2', ...]]  \n",
        "      y = data['target_variable']  \n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "      ```\n",
        "\n",
        "25. Explain data encoding?\n",
        "  - Data encoding is the process of converting data from one form to another, usually for the purpose of transmission, storage, or analysis."
      ],
      "metadata": {
        "id": "OHvslfQraVHH"
      }
    }
  ]
}